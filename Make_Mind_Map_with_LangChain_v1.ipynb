{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1OqX5Licb1zJEWQ5SzBo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shu65/langchain_examples/blob/main/Make_Mind_Map_with_LangChain_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snFhXNKLAHDY",
        "outputId": "00c9d5ff-3a4e-465c-f3b1-e107deab6261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.9/dist-packages (0.0.141)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.9/dist-packages (3.8.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.9/dist-packages (0.3.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: SQLAlchemy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.47)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: gptcache>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from langchain) (0.1.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from pypdf) (4.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (2.0.12)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from gptcache>=0.1.7->langchain) (5.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2,>=1->langchain) (2.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai langchain python-dotenv transformers pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDMK1XvVAawn",
        "outputId": "cc4424c8-d5c5-4b12-cce1-c317cbbb7c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain                     0.0.141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IWV5e2OAext",
        "outputId": "9e515de1-03b8-4690-87a9-ef46fe14b996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/colab_env/lang_chain_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9AVwbJUAhg0",
        "outputId": "609d28dd-86fa-4c0b-d5bb-f5326397c408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/colab_env/lang_chain_env\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\"/content/drive/MyDrive/colab_env/lang_chain_env\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wtkTs2CAn4S",
        "outputId": "a2972506-480d-4d2d-d77a-08ff59ca4fa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download paper\n",
        "\n",
        "!wget --user-agent TryToStopMeFromUsingWgetNow https://arxiv.org/pdf/2205.14135.pdf -O reserch_paper.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vvthVoQAqun",
        "outputId": "57a9b510-d1b6-4a74-d9f3-faeaa128dbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-17 11:57:07--  https://arxiv.org/pdf/2205.14135.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
            "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2630825 (2.5M) [application/pdf]\n",
            "Saving to: ‘reserch_paper.pdf’\n",
            "\n",
            "reserch_paper.pdf   100%[===================>]   2.51M  4.81MB/s    in 0.5s    \n",
            "\n",
            "2023-04-17 11:57:08 (4.81 MB/s) - ‘reserch_paper.pdf’ saved [2630825/2630825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.pdf import PyPDFLoader\n",
        "\n",
        "file_path = \"./reserch_paper.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "pages = loader.load()\n",
        "print(pages[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lMnLE18BZVC",
        "outputId": "99af9257-7ff2-44c9-89c3-40c639459f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlashAttention : Fast and Memory-Eﬃcient Exact Attention\n",
            "with IO-Awareness\n",
            "Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz, and Christopher Réy\n",
            "yDepartment of Computer Science, Stanford University\n",
            "zDepartment of Computer Science and Engineering, University at Buﬀalo, SUNY\n",
            "{trid,danfu}@cs.stanford.edu ,ermon@stanford.edu ,atri@buffalo.edu ,\n",
            "chrismre@cs.stanford.edu\n",
            "June 24, 2022\n",
            "Abstract\n",
            "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity\n",
            "of self-attention are quadratic in sequence length. Approximate attention methods have attempted\n",
            "to address this problem by trading oﬀ model quality to reduce the compute complexity, but often do\n",
            "not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-\n",
            "aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\n",
            "an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\n",
            "between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\n",
            "ofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\n",
            "optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\n",
            "an approximate attention algorithm that is faster than any existing approximate attention method.\n",
            "FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\n",
            "on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \u0002speedup on\n",
            "GPT-2 (seq. length 1K), and 2.4 \u0002speedup on long-range arena (seq. length 1K-4K). FlashAttention\n",
            "and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models\n",
            "(0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new\n",
            "capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge\n",
            "(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n",
            "1 Introduction\n",
            "Transformer models [ 82] have emerged as the most widely used architecture in applications such as natural\n",
            "language processing and image classiﬁcation. Transformers have grown larger [ 5] and deeper [ 83], but\n",
            "equipping them with longer context remains diﬃcult [ 80], since the self-attention module at their heart\n",
            "has time and memory complexity quadratic in sequence length. An important question is whether making\n",
            "attention faster and more memory-eﬃcient can help Transformer models address their runtime and memory\n",
            "challenges for long sequences.\n",
            "Many approximate attention methods have aimed to reduce the compute and memory requirements of\n",
            "attention. These methods range from sparse-approximation [ 51,74] to low-rank approximation [ 12,50,84],\n",
            "and their combinations [ 3,9,92]. Although these methods reduce the compute requirements to linear or\n",
            "near-linear in sequence length, many of them do not display wall-clock speedup against standard attention\n",
            "and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not\n",
            "correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).\n",
            "In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is,\n",
            "carefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU\n",
            "on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [ 45], Figure 1 left). On modern\n",
            "1arXiv:2205.14135v2  [cs.LG]  23 Jun 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "pages = pages[:11]\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"p50k_base\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "sub_pages = text_splitter.split_documents(pages)\n",
        "len(sub_pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bACBKRFFCVc2",
        "outputId": "e2b025f9-1a5b-4bea-c549-b3b1a2d0683d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import langchain\n",
        "from langchain import OpenAI\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.llms.fake import FakeListLLM\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.schema import BaseLanguageModel"
      ],
      "metadata": {
        "id": "xojyr4DrDJW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAKE_MIND_MAP_PROMPT_TEMPLATE_STR = \"\"\"I would like to create a mind map using the Xmind tool about following text.\n",
        "Can you provide me with some text in Markdown format that is compatible with Xmind? \n",
        "Please include a Central Topic with Main Topics and any additional information goes to Subtopics that will help create an effective mind map.\"\n",
        "\n",
        "The mindmap format is follow:\n",
        "\n",
        "# Central Tpoic Title\n",
        "\n",
        "## Main Topic Title 1\n",
        "\n",
        "### Subtopic Title 1\n",
        "\n",
        "- Subtopic Title 1\n",
        "\n",
        "  - Subtopic Title 1\n",
        "  - Subtopic Title 2\n",
        "\n",
        "### Subtopic Title 2\n",
        "\n",
        "- Subtopic Title 1\n",
        "\n",
        "  - Subtopic Title 1\n",
        "  - Subtopic Title 2\n",
        "\n",
        "## Main Topic Title 2\n",
        "\n",
        "- Subtopic Title 1\n",
        "\n",
        "  - Subtopic Title 1\n",
        "  - Subtopic Title 2\n",
        "\n",
        "\n",
        "Text:\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "Mind map in Markdown format:\"\"\"\n",
        "MAKE_MIND_MAP_PROMPT_TEMPLATE = PromptTemplate(\n",
        "    template=MAKE_MIND_MAP_PROMPT_TEMPLATE_STR, input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "class MindMapMaker(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm: BaseLanguageModel,\n",
        "        verbose: Optional[bool] = None,\n",
        "    ) -> None:\n",
        "        self.llm_chain = LLMChain(\n",
        "            llm=llm, prompt=MAKE_MIND_MAP_PROMPT_TEMPLATE, verbose=verbose\n",
        "        )\n",
        "        assert len(self.llm_chain.prompt.input_variables) == 1\n",
        "\n",
        "    def __call__(self, docs: List[Document]) -> List[Document]:\n",
        "        results = self.llm_chain.apply(\n",
        "            [\n",
        "                {**{self.llm_chain.prompt.input_variables[0]: d.page_content}}\n",
        "                for d in docs\n",
        "            ]\n",
        "        )\n",
        "        question_result_key = self.llm_chain.output_key\n",
        "        ret = [\n",
        "            Document(page_content=r[question_result_key], metadata=docs[i].metadata)\n",
        "            for i, r in enumerate(results)\n",
        "        ]\n",
        "        return ret"
      ],
      "metadata": {
        "id": "ILrBWoflC1yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MERGE_PROMPT_TEMPLATE_STR = \"\"\"Compact the following mind map into one mind maps. \n",
        "The format should be Xmind compatible Markdwon format.\n",
        "The mind map format is follow:\n",
        "\n",
        "# Central Tpoic Title\n",
        "\n",
        "## Main Topic Title 1\n",
        "\n",
        "### Subtopic Title 1\n",
        "\n",
        "- Subtopic Title 1\n",
        "\n",
        "  - Subtopic Title 1\n",
        "  - Subtopic Title 2\n",
        "\n",
        "### Subtopic Title 2\n",
        "\n",
        "- Subtopic Title 1\n",
        "\n",
        "  - Subtopic Title 1\n",
        "  - Subtopic Title 2\n",
        "\n",
        "## Main Topic Title 2\n",
        "\n",
        "- Subtopic Title 1\n",
        "\n",
        "  - Subtopic Title 1\n",
        "  - Subtopic Title 2\n",
        "\n",
        "\n",
        "{text}\n",
        "\n",
        "\n",
        "Output mind map in Markdown format:\"\"\"\n",
        "MERGE_PROMPT_TEMPLATE = PromptTemplate(\n",
        "    template=MERGE_PROMPT_TEMPLATE_STR, input_variables=[\"text\"]\n",
        ")\n",
        "\n",
        "class MindMapMerger(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        llm: BaseLanguageModel,\n",
        "        input_token_max: int = 2000,\n",
        "        return_intermediate_steps=True,\n",
        "        verbose: Optional[bool] = None,\n",
        "    ) -> None:\n",
        "        self.llm_chain = LLMChain(\n",
        "            llm=llm, prompt=MERGE_PROMPT_TEMPLATE, verbose=verbose\n",
        "        )\n",
        "        self.input_token_max = input_token_max\n",
        "        self.return_intermediate_steps = return_intermediate_steps\n",
        "        assert len(self.llm_chain.prompt.input_variables) == 1\n",
        "\n",
        "    def _get_input_text(self, docs: List[Document]) -> str:\n",
        "        doc_strings_values = []\n",
        "        for i, doc in enumerate(docs):\n",
        "            doc_strings_values.append(f\"Input mind map {i}\")\n",
        "            doc_strings_values.append(doc.page_content.strip())\n",
        "        return \"\\n\\n\".join(doc_strings_values)\n",
        "\n",
        "    def _get_output_metadata(self, docs: List[Document]) -> dict:\n",
        "        ret = {}\n",
        "        for i, doc in enumerate(docs):\n",
        "            ret[i] = doc.metadata\n",
        "        return ret\n",
        "\n",
        "    def __call__(self, docs: List[Document]) -> Tuple[Document, dict]:\n",
        "        input_docs = docs\n",
        "        if self.return_intermediate_steps:\n",
        "            extra_return_dict = {\"inputs\": input_docs}\n",
        "        else:\n",
        "            extra_return_dict = {}\n",
        "\n",
        "        length_func = self.llm_chain.llm.get_num_tokens\n",
        "        num_tokens_per_doc = [\n",
        "            length_func(input_docs[i].page_content) for i in range(len(input_docs))\n",
        "        ]\n",
        "        merge_step = 0\n",
        "        while len(input_docs) > 1:\n",
        "            print(f\"merge_step: {merge_step}, number of mind maps: {len(input_docs)}\")\n",
        "            current_token = 0\n",
        "            merge_inputs_list = []\n",
        "            merge_inputs = []\n",
        "            for doc_i in range(len(input_docs)):\n",
        "                if (current_token + num_tokens_per_doc[doc_i]) >= self.input_token_max:\n",
        "                    if len(merge_inputs) <= 1:\n",
        "                        raise ValueError(\n",
        "                            \"Can not merege two mind maps, because the total length of mind maps are too long.\"\n",
        "                            f\"One mind map lenght is {current_token}. \"\n",
        "                            f\"The other mind map lengtht is {num_tokens_per_doc[doc_i]}.\"\n",
        "                            f\"token_max is {self.input_token_max}\"\n",
        "                        )\n",
        "                    merge_inputs_list.append(merge_inputs)\n",
        "                    merge_inputs = []\n",
        "                    current_token = 0\n",
        "                merge_inputs.append(input_docs[doc_i])\n",
        "                current_token += num_tokens_per_doc[doc_i]\n",
        "\n",
        "            if len(merge_inputs) > 0:\n",
        "                merge_inputs_list.append(merge_inputs)\n",
        "\n",
        "            new_input_docs = []\n",
        "            for merge_inputs in merge_inputs_list:\n",
        "                if len(merge_inputs) > 1:\n",
        "                    input_text = self._get_input_text(merge_inputs)\n",
        "                    result = self.llm_chain.run(\n",
        "                        **{self.llm_chain.prompt.input_variables[0]: input_text}\n",
        "                    )\n",
        "                    metadata = self._get_output_metadata(merge_inputs)\n",
        "                    new_doc = Document(page_content=result, metadata=metadata)\n",
        "                    new_input_docs.append(new_doc)\n",
        "                else:\n",
        "                    new_input_docs.append(merge_inputs[0])\n",
        "\n",
        "            if self.return_intermediate_steps:\n",
        "                extra_return_dict.update({f\"merge_step{merge_step}\": new_input_docs})\n",
        "            input_docs = new_input_docs\n",
        "            num_tokens_per_doc = [\n",
        "                length_func(input_docs[i].page_content) for i in range(len(input_docs))\n",
        "            ]\n",
        "            merge_step += 1\n",
        "\n",
        "        return input_docs[0], extra_return_dict"
      ],
      "metadata": {
        "id": "XstrSO_WDU6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_page_content(value):\n",
        "    if isinstance(value, Document):\n",
        "        return value.page_content\n",
        "    elif isinstance(value, dict):\n",
        "        ret = {k: extract_page_content(v) for k, v in value.items()}\n",
        "        return ret\n",
        "    elif isinstance(value, list):\n",
        "        ret = [extract_page_content(v) for v in value]\n",
        "        return ret\n",
        "    else:\n",
        "        raise RuntimeError(f\"{type(value)}  is not support\")"
      ],
      "metadata": {
        "id": "-TOLz06jDbPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mind_map_maker_llm = OpenAI(temperature=0, max_tokens=256)\n",
        "mind_map_merger_llm = OpenAI(temperature=0, max_tokens=512)\n",
        "\n",
        "mind_map_maker = MindMapMaker(llm=mind_map_maker_llm)\n",
        "mind_map_merger = MindMapMerger(llm=mind_map_merger_llm, input_token_max=1200)\n",
        "mind_maps = mind_map_maker(sub_pages)\n",
        "\n",
        "merged_mind_map, history = mind_map_merger(mind_maps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZOP0aXZDf9Q",
        "outputId": "8bd07073-a542-492d-bf59-8661b0bfa64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merge_step: 0, number of mind maps: 21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.openai:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\r\n",
            "<head><title>502 Bad Gateway</title></head>\r\n",
            "<body>\r\n",
            "<center><h1>502 Bad Gateway</h1></center>\r\n",
            "<hr><center>cloudflare</center>\r\n",
            "</body>\r\n",
            "</html>\r\n",
            ").\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merge_step: 1, number of mind maps: 4\n",
            "merge_step: 2, number of mind maps: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mind Map:\")\n",
        "print(merged_mind_map.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB7VPyhtDxZm",
        "outputId": "71dae918-b59d-4b83-88ef-81f0ffcff4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mind Map:\n",
            "\n",
            "\n",
            "# FlashAttention : Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
            "\n",
            "## Introduction\n",
            "\n",
            "### FLOP Reduction\n",
            "\n",
            "- Sparse-Approximation\n",
            "- Low-Rank Approximation\n",
            "- Combinations\n",
            "- IO-Awareness\n",
            "  - Reads and Writes\n",
            "  - Fast and Slow Memory\n",
            "  - GPU On-Chip SRAM and HBM\n",
            "\n",
            "## Proposed Method\n",
            "\n",
            "### FlashAttention\n",
            "\n",
            "- Tiling\n",
            "- Reduce Memory Reads/Writes\n",
            "- Analyze IO Complexity\n",
            "- Bandwidth & Memory Size\n",
            "- Attention on GPT-2\n",
            "  - FlashAttention PyTorch\n",
            "    - Time (ms)\n",
            "    - Matmul\n",
            "    - Mask\n",
            "    - Softmax\n",
            "    - Dropout\n",
            "    - Matmul\n",
            "    - Fused Kernel\n",
            "    - Q: N x d\n",
            "    - V: N X d\n",
            "    - KT: d x N\n",
            "    - QKT: N x N\n",
            "    - sm(Q KT)V: N x d\n",
            "- Outer Loop\n",
            "  - Copy Block to SRAM\n",
            "  - Copy\n",
            "    - Outer Loop\n",
            "    - Inner Loop\n",
            "  - Compute Block on SRAM\n",
            "  - Output to HBM\n",
            "- Inner Loop\n",
            "  - Outer Loop\n",
            "  - GPU\n",
            "    - SRAM\n",
            "    - GPU\n",
            "    - HBM\n",
            "    - Main Memory (CPU DRAM)\n",
            "  - SRAM : 19 TB/s (20 MB)\n",
            "  - HBM: 1.5 TB/s (40 GB)\n",
            "  - DRAM : 12.8 GB/s (>1 TB)\n",
            "- Fewer HBM Accesses\n",
            "  - Fig. 2\n",
            "  - Lower Bound\n",
            "- Memory Access Overhead\n",
            "  - Block-Sparse FlashAttention\n",
            "    - 2-4x Faster\n",
            "    - Sequence Length of 64k\n",
            "    - IO Complexity Better than FlashAttention\n",
            "- Softmax Decomposition\n",
            "  - Vectors 𝑥¹1º𝑥¹2º2R𝐵\n",
            "    - Softmax of Concatenated 𝑥\n",
            "      - Decomposition\n",
            "        - 𝑚¹𝑥º\n",
            "        - 𝑓¹𝑥º\n",
            "        - ℓ¹𝑥º\n",
            "- Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"mind_map_merge_history.josn\", \"w\") as f:\n",
        "    json.dump(extract_page_content(history), f, indent=2)"
      ],
      "metadata": {
        "id": "Ea9fC3ffDtKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(mind_maps)):\n",
        "  print(f\"sub page {i}\")\n",
        "  print(mind_maps[i].page_content)\n",
        "  print(\"-\"*10)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYSEj90oHwpt",
        "outputId": "3b39e4f0-8098-4c6c-d583-1796e4c29737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sub page 0\n",
            "\n",
            "\n",
            "# FlashAttention : Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
            "\n",
            "## Introduction\n",
            "\n",
            "### FLOP Reduction\n",
            "\n",
            "- Sparse-Approximation\n",
            "- Low-Rank Approximation\n",
            "- Combinations\n",
            "\n",
            "### IO-Awareness\n",
            "\n",
            "- Reads and Writes\n",
            "- Fast and Slow Memory\n",
            "- GPU On-Chip SRAM and HBM\n",
            "\n",
            "## Proposed Method\n",
            "\n",
            "### FlashAttention\n",
            "\n",
            "- Tiling\n",
            "- Reduce Memory Reads/Writes\n",
            "- Analyze IO Complexity\n",
            "\n",
            "### Block-Sparse FlashAttention\n",
            "\n",
            "- Faster than Existing Approximate Attention\n",
            "- End-to-End Wall-Clock Speedup\n",
            "- Longer Context in Transformers\n",
            "- Higher Quality Models\n",
            "- New Capabilities\n",
            "----------\n",
            "\n",
            "sub page 1\n",
            "\n",
            "\n",
            "# FlashAttention Memory Hierarchy\n",
            "\n",
            "## Bandwidth & Memory Size\n",
            "\n",
            "### Attention on GPT-2\n",
            "\n",
            "- FlashAttention PyTorch\n",
            "  - Time (ms)\n",
            "  - Matmul\n",
            "  - Mask\n",
            "  - Softmax\n",
            "  - Dropout\n",
            "  - Matmul\n",
            "  - Fused Kernel\n",
            "  - Q: N x d\n",
            "  - V: N X d\n",
            "  - KT: d x N\n",
            "  - QKT: N x N\n",
            "  - sm(Q KT)V: N x d\n",
            "\n",
            "### Outer Loop\n",
            "\n",
            "- Copy Block to SRAM\n",
            "- Copy\n",
            "  - Outer Loop\n",
            "  - Inner Loop\n",
            "- Compute Block on SRAM\n",
            "- Output to HBM\n",
            "\n",
            "### Inner Loop\n",
            "\n",
            "- Outer Loop\n",
            "- GPU\n",
            "  - SRAM\n",
            "  - GPU\n",
            "  - HBM\n",
            "  - Main Memory (CPU DRAM)\n",
            "- SRAM : 19 TB/s (20 MB)\n",
            "- HBM: 1.5 TB/s (40 GB)\n",
            "- DRAM : 12.8 GB/s (>1 TB)\n",
            "----------\n",
            "\n",
            "sub page 2\n",
            "\n",
            "\n",
            "# FlashAttention\n",
            "\n",
            "## Standard Attention\n",
            "\n",
            "### Fewer HBM Accesses\n",
            "\n",
            "- Fig. 2\n",
            "- Lower Bound\n",
            "\n",
            "## Memory Access Overhead\n",
            "\n",
            "### Block-Sparse FlashAttention\n",
            "\n",
            "- 2-4x Faster\n",
            "- Sequence Length of 64k\n",
            "- IO Complexity Better than FlashAttention\n",
            "\n",
            "## Extensions\n",
            "\n",
            "- Attention on Multi-GPU\n",
            "- Kernel Regression\n",
            "- Block-Sparse Matrix\n",
            "----------\n",
            "\n",
            "sub page 3\n",
            "\n",
            "\n",
            "# Attention Mechanisms\n",
            "\n",
            "## Hardware Performance\n",
            "\n",
            "### GPU Memory Hierarchy\n",
            "\n",
            "- HBM\n",
            "- On-chip SRAM\n",
            "\n",
            "### Execution Model\n",
            "\n",
            "- Load inputs from HBM to registers and SRAM\n",
            "- Compute\n",
            "- Write outputs to HBM\n",
            "\n",
            "### Performance Characteristics\n",
            "\n",
            "- Compute-bound\n",
            "  - Matrix multiply with large inner dimension\n",
            "  - Convolution with large number of channels\n",
            "- Memory-bound\n",
            "  - Elementwise operations\n",
            "  - Reduction operations\n",
            "\n",
            "## Kernel Fusion\n",
            "\n",
            "- Load input once from HBM\n",
            "- Accelerate memory-bound operations\n",
            "----------\n",
            "\n",
            "sub page 4\n",
            "\n",
            "\n",
            "# Accelerating Memory-Bound Operations\n",
            "\n",
            "## Kernel Fusion\n",
            "- Elementwise Operations\n",
            "  - Activation\n",
            "  - Dropout\n",
            "- Reduction Operations\n",
            "  - Sum\n",
            "  - Softmax\n",
            "  - Batch Norm\n",
            "  - Layer Norm\n",
            "- Compilers Automatically Fuse Many Elementwise Operations\n",
            "\n",
            "## Flash Attention\n",
            "- Code Available at https://github.com/HazyResearch/flash-attention\n",
            "----------\n",
            "\n",
            "sub page 5\n",
            "\n",
            "\n",
            "# Attention Implementation\n",
            "\n",
            "## Standard Attention Implementation\n",
            "\n",
            "### Input Sequences\n",
            "- Q\n",
            "- K\n",
            "- V\n",
            "\n",
            "### Output\n",
            "- S\n",
            "- P\n",
            "- O\n",
            "\n",
            "### Algorithm\n",
            "- Load Q-K by blocks\n",
            "- Compute S=QK>\n",
            "- Read S from HBM\n",
            "- Compute P=softmax¹Sº\n",
            "- Load P and V by blocks\n",
            "- Compute O=PV\n",
            "- Return O\n",
            "\n",
            "## FlashAttention\n",
            "\n",
            "### Algorithm\n",
            "- Split inputs Q-K-V into blocks\n",
            "- Load from slow HBM to fast SRAM\n",
            "- Compute attention output with respect to blocks\n",
            "- Scale output of each block by normalization factor\n",
            "\n",
            "### Analysis\n",
            "- HBM accesses sub-quadratic in N\n",
            "- Fewer HBM accesses compared to standard attention\n",
            "\n",
            "### Extensions\n",
            "- Block-sparse attention\n",
            "----------\n",
            "\n",
            "sub page 6\n",
            "\n",
            "\n",
            "# Maximum Likelihood Estimation\n",
            "\n",
            "## Main Topic 1: Definition\n",
            "\n",
            "### Subtopic 1: Definition\n",
            "\n",
            "- Definition of Maximum Likelihood Estimation (MLE): \n",
            "  - MLE is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters.\n",
            "\n",
            "### Subtopic 2: Formula\n",
            "\n",
            "- Formula for MLE: \n",
            "  - MLE is expressed as: 𝑚¹𝑥º:=max 𝑖𝑥𝑖 𝑓¹𝑥º:=\u0002 𝑒𝑥1\u0000𝑚¹𝑥º 𝑒𝑥𝐵\u0000𝑚¹𝑥º\u0003 ℓ¹𝑥º:=∑︁ 𝑖𝑓¹𝑥º𝑖softmax¹𝑥º:=𝑓¹𝑥ºℓ¹𝑥º\n",
            "\n",
            "## Main Topic 2:\n",
            "----------\n",
            "\n",
            "sub page 7\n",
            "\n",
            "\n",
            "# Softmax Decomposition\n",
            "\n",
            "## Main Topic 1: Vectors 𝑥¹1º𝑥¹2º2R𝐵\n",
            "\n",
            "### Subtopic 1: Softmax of Concatenated 𝑥\n",
            "\n",
            "- Subtopic 1: Decomposition\n",
            "\n",
            "  - Subtopic 1: 𝑚¹𝑥º\n",
            "  - Subtopic 2: 𝑓¹𝑥º\n",
            "  - Subtopic 3: ℓ¹𝑥º\n",
            "  - Subtopic 4: Softmax\n",
            "\n",
            "## Main Topic 2: Recomputation\n",
            "\n",
            "- Subtopic 1: Backward Pass\n",
            "\n",
            "  - Subtopic 1: Matrices SP2R𝑁\u0002𝑁\n",
            "  - Subtopic 2: Output O\n",
            "  - Subtopic 3: Softmax Normalization Statistics ¹𝑚ℓº\n",
            "\n",
            "## Main Topic 3: Implementation Details\n",
            "\n",
            "- Subtopic 1: Kernel Fusion\n",
            "\n",
            "  - Subtopic 1: Tiling\n",
            "  - Subtopic 2: CUDA Kernel\n",
            "  - Subtopic 3: HBM Accesses\n",
            "----------\n",
            "\n",
            "sub page 8\n",
            "\n",
            "\n",
            "# FlashAttention\n",
            "\n",
            "## Initialize\n",
            "- O=¹0º𝑁\u0002𝑑2R𝑁\u0002𝑑\n",
            "- ℓ=¹0º𝑁2R𝑁\n",
            "- 𝑚=¹\u00001º𝑁2R𝑁in HBM\n",
            "\n",
            "## Divide\n",
            "- Qinto𝑇𝑟=l\n",
            "𝑁\n",
            "𝐵𝑟m blocks Q1Q𝑇𝑟of size𝐵𝑟\u0002𝑑each\n",
            "- KVin to𝑇𝑐=l\n",
            "𝑁\n",
            "𝐵𝑐m blocks K1K𝑇𝑐andV1V𝑇𝑐, of size𝐵𝑐\u0002𝑑each\n",
            "- Ointo𝑇𝑟blocks O𝑖O𝑇𝑟of size𝐵\n",
            "----------\n",
            "\n",
            "sub page 9\n",
            "\n",
            "\n",
            "# FlashAttention\n",
            "\n",
            "## Theorem 1\n",
            "- Algorithm 1 returns O=softmax¹QK>ºVwith𝑂¹𝑁2𝑑ºFLOPs and requires 𝑂¹𝑁ºadditional memory beyond inputs and output.\n",
            "\n",
            "## Analysis: IO Complexity of FlashAttention\n",
            "- Signiﬁcant reduction in HBM accesses compared to standard attention\n",
            "- Lower bound, proving that no exact attention algorithm can\n",
            "----------\n",
            "\n",
            "sub page 10\n",
            "\n",
            "\n",
            "# Attention Standard FlashAttention\n",
            "\n",
            "## GFLOPs\n",
            "\n",
            "- 66.6\n",
            "- 75.2\n",
            "\n",
            "## HBM R/W (GB)\n",
            "\n",
            "- 40.3\n",
            "- 4.4\n",
            "\n",
            "## Runtime (ms)\n",
            "\n",
            "- 41.7\n",
            "- 7.3\n",
            "\n",
            "## Sparsity Speedup\n",
            "\n",
            "### % Non-Zero Blocks\n",
            "\n",
            "- 20\n",
            "- 60\n",
            "- 50\n",
            "- 100\n",
            "- 150\n",
            "\n",
            "### Fwd + Bwd (ms)\n",
            "\n",
            "### Effect of Block Size\n",
            "\n",
            "#### Block Size\n",
            "\n",
            "- 64\n",
            "- 128\n",
            "- 256\n",
            "- 512\n",
            "\n",
            "#### Fwd Runtime (ms)\n",
            "\n",
            "- 6\n",
            "- 2\n",
            "\n",
            "#### HBM Accesses (GB)\n",
            "\n",
            "##### Dense\n",
            "\n",
            "##### FlashAttention\n",
            "\n",
            "##### Block-Sparse\n",
            "\n",
            "##### FlashAttention\n",
            "\n",
            "- 2\n",
            "- 4\n",
            "- 6\n",
            "\n",
            "## Runtime\n",
            "\n",
            "- HBM Accesses\n",
            "\n",
            "## Figure 2\n",
            "\n",
            "### Left\n",
            "\n",
            "- Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium\n",
            "- seq. length 1024, head dim. 64, 16 heads, batch size 64\n",
            "\n",
            "----------\n",
            "\n",
            "sub page 11\n",
            "\n",
            "\n",
            "# Attention Run-Time\n",
            "\n",
            "## HBM Accesses\n",
            "\n",
            "### Lower Bound\n",
            "- Subtopic Title 1\n",
            "  - Proving Parameterized Complexity Lower Bounds\n",
            "\n",
            "### Validation\n",
            "- Subtopic Title 1\n",
            "  - FlashAttention vs Standard Attention\n",
            "  - Varying Block Size\n",
            "\n",
            "## Extension: Block-Sparse FlashAttention\n",
            "- Subtopic Title 1\n",
            "  - Computing S, P, O\n",
            "  - Block Form of Mask Matrix\n",
            "----------\n",
            "\n",
            "sub page 12\n",
            "\n",
            "\n",
            "# FlashAttention\n",
            "\n",
            "## Algorithm\n",
            "\n",
            "### Algorithm 1\n",
            "\n",
            "- Algorithm 1\n",
            "\n",
            "### Algorithm 5\n",
            "\n",
            "- Algorithm 5\n",
            "\n",
            "## IO Complexity\n",
            "\n",
            "- Proposition 4\n",
            "\n",
            "## Experiments\n",
            "\n",
            "### Training Speed\n",
            "\n",
            "- FlashAttention outperforms MLPerf 1.1\n",
            "- Speeds up GPT-2\n",
            "- Speeds up LRA benchmark\n",
            "\n",
            "### Quality\n",
            "\n",
            "- Scales Transformers to longer sequences\n",
            "- Better perplexity\n",
            "- 6.4 points of lift on two long-document classification tasks\n",
            "- Better-than-random performance on Path-X\n",
            "- Block-sparse FlashAttention yields better-than-random performance on Path-256\n",
            "\n",
            "### Benchmarking Attention\n",
            "\n",
            "- Memory footprint scales linearly with sequence length\n",
            "- Faster than standard attention for common sequence lengths\n",
            "- Runtime of block-sparse FlashAttention scales linearly in sequence length\n",
            "- Faster than all existing approximate attention baselines\n",
            "\n",
            "### Faster Models with FlashAttention\n",
            "\n",
            "- BERT\n",
            "- GPT-2\n",
            "----------\n",
            "\n",
            "sub page 13\n",
            "\n",
            "\n",
            "# Training Time of BERT-Large\n",
            "\n",
            "## Nvidia MLPerf 1.1\n",
            "- Training Time (minutes): 20.0 ± 1.5\n",
            "\n",
            "## FlashAttention (Ours)\n",
            "- Training Time (minutes): 17.4 ± 1.4\n",
            "\n",
            "## GPT-2\n",
            "- HuggingFace\n",
            "  - End-to-end Speedup: up to 3x\n",
            "- Megatron-LM\n",
            "  - Speedup: 1.7x\n",
            "----------\n",
            "\n",
            "sub page 14\n",
            "\n",
            "\n",
            "# GPT-2 Model Implementations\n",
            "\n",
            "## Speedup\n",
            "\n",
            "### Huggingface\n",
            "- GPT-2 small: 9.5 days (1.0 \u0002)\n",
            "\n",
            "### Megatron-LM\n",
            "- GPT-2 small: 4.7 days (2.0 \u0002)\n",
            "- GPT-2 medium: 11.5 days (1.8 \u0002)\n",
            "\n",
            "### FlashAttention\n",
            "- GPT-2 small: 2.7 days (3.5\u0002)\n",
            "- GPT-2 medium: 6.9 days (3.0\u0002)\n",
            "\n",
            "## Long-Range Arena\n",
            "\n",
            "### Standard Attention\n",
            "- Accuracy\n",
            "- Throughput\n",
            "- Training Time\n",
            "\n",
            "### FlashAttention\n",
            "- Accuracy\n",
            "- Throughput\n",
            "- Training Time\n",
            "- Speedup (2.4\u0002)\n",
            "\n",
            "### Block-sparse FlashAttention\n",
            "- Accuracy\n",
            "- Throughput\n",
            "- Training Time\n",
            "- Speedup (2.8\u0002)\n",
            "\n",
            "### Approximate Attention Baselines\n",
            "- Linformer\n",
            "- Linear Attention\n",
            "- Performer\n",
            "- Local Attention\n",
            "- Reformer\n",
            "- Smyrf\n",
            "\n",
            "## Longer Sequences\n",
            "\n",
            "### GPT-2 small\n",
            "----------\n",
            "\n",
            "sub page 15\n",
            "\n",
            "\n",
            "# Faster Training Time on 8 A100 GPUs\n",
            "\n",
            "## Model Implementations\n",
            "\n",
            "### Context Length\n",
            "\n",
            "- OpenWebText (ppl)\n",
            "\n",
            "### Training Time (speedup)\n",
            "\n",
            "- GPT-2 small - Megatron-LM 1k\n",
            "- GPT-2 small - FlashAttention 1k\n",
            "- GPT-2 small - FlashAttention 2k\n",
            "- GPT-2 small - FlashAttention 4k\n",
            "\n",
            "## Long Document Classification\n",
            "\n",
            "- Training Transformers with longer sequences with FlashAttention\n",
            "- Improves performance on the MIMIC-III and ECtHR datasets\n",
            "- MIMIC-III contains intensive care unit patient discharge summaries\n",
            "- ECtHR contains legal cases from the 3LRA\n",
            "- Accuracy results are known to be highly dependent on the tuning procedure\n",
            "- Reproduced baselines perform better than as reported in the original comparison\n",
            "----------\n",
            "\n",
            "sub page 16\n",
            "\n",
            "\n",
            "# Attention Memory Usage\n",
            "\n",
            "## Sequence Length\n",
            "\n",
            "### Runtime (Fwd Pass + Bwd Pass)\n",
            "\n",
            "- Subtopic Title 1\n",
            "\n",
            "  - FlashAttention\n",
            "  - Block-Sparse FlashAttention\n",
            "  - PyTorch Attention\n",
            "  - Megatron Attention\n",
            "  - Linformer Attention\n",
            "  - OpenAI Sparse Attention\n",
            "\n",
            "### Memory Footprint (GB)\n",
            "\n",
            "- Subtopic Title 1\n",
            "\n",
            "  - 256\n",
            "  - 8K\n",
            "  - 16K\n",
            "  - 32K\n",
            "  - 64K\n",
            "  - 128\n",
            "  - 256\n",
            "  - 512\n",
            "  - 1024\n",
            "  - 2048\n",
            "  - 4096\n",
            "\n",
            "## European Court of Human Rights\n",
            "\n",
            "- Subtopic Title 1\n",
            "\n",
            "  - Average number of tokens\n",
            "  - Longest document\n",
            "  - Evaluate lift from increasing sequence length\n",
            "\n",
            "## Table 5\n",
            "\n",
            "- Subtopic Title 1\n",
            "\n",
            "  - Runtime of forward pass + backward pass\n",
            "  - Attention memory usage\n",
            "\n",
            "## Table 6\n",
            "\n",
            "- Subtopic Title 1\n",
            "\n",
            "  - Transformer models\n",
            "  - Linformer\n",
            "  - Linear Attention\n",
            "  - Performer\n",
            "  - Local Attention\n",
            "  - Reformer\n",
            "  - SMYRF\n",
            "  -\n",
            "----------\n",
            "\n",
            "sub page 17\n",
            "\n",
            "\n",
            "# Runtime and Memory Footprint of FlashAttention\n",
            "\n",
            "## Runtime\n",
            "\n",
            "### FlashAttention\n",
            "- 3x faster than PyTorch implementation\n",
            "- Quadratic growth with sequence length\n",
            "\n",
            "### Approximate/Sparse Attention\n",
            "- Linear growth with sequence length\n",
            "- Crossover with FlashAttention at sequences between 512 and 1024\n",
            "\n",
            "### Block-Sparse FlashAttention\n",
            "- Faster than all implementations of exact, sparse, and approximate attention\n",
            "\n",
            "## Memory Footprint\n",
            "\n",
            "### FlashAttention\n",
            "- Linear growth with sequence length\n",
            "- Up to 20x more memory efficient than exact attention baselines\n",
            "\n",
            "### Approximate Attention\n",
            "- More memory efficient than approximate attention baselines\n",
            "\n",
            "### Block-Sparse FlashAttention\n",
            "- Same memory footprint as FlashAttention\n",
            "- More memory efficient than Linformer\n",
            "\n",
            "## Limitations and Future Directions\n",
            "\n",
            "### Compiling to CUDA\n",
            "- Writing attention algorithm in lower-level language than PyTorch\n",
            "- Need for a method that supports writing attention algorithms in high-level language\n",
            "\n",
            "### IO-Aware Deep Learning\n",
            "- Extend beyond attention\n",
            "\n",
            "### Multi-GPU IO-Aware Methods\n",
            "- Parallelizable across multiple GPUs\n",
            "- Account for data\n",
            "----------\n",
            "\n",
            "sub page 18\n",
            "\n",
            "\n",
            "# Supervision\n",
            "\n",
            "## ONR N00014-20-1-2480\n",
            "\n",
            "### Understanding and Applying Non-Euclidean Geometry in Machine Learning\n",
            "\n",
            "### N000142012275 (NEPTUNE)\n",
            "\n",
            "### NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total\n",
            "\n",
            "### HAI-GCP & HAI-Azure Cloud Credits for Research program\n",
            "\n",
            "### Stanford Data Science Initiative (SDSI)\n",
            "\n",
            "### Department of Defense (DoD)\n",
            "\n",
            "### National Defense Science and Engineering Graduate Fellowship (NDSEG) Program\n",
            "\n",
            "### Stanford DAWN project: Facebook, Google, and VMWare\n",
            "\n",
            "### U.S. Government\n",
            "----------\n",
            "\n",
            "sub page 19\n",
            "\n",
            "\n",
            "# Natural Language Processing\n",
            "\n",
            "## Text Preprocessing\n",
            "\n",
            "### Tokenization\n",
            "\n",
            "- Tokenization\n",
            "  - Word Tokenization\n",
            "  - Sentence Tokenization\n",
            "\n",
            "### Stopwords Removal\n",
            "\n",
            "- Stopwords Removal\n",
            "  - Identifying Stopwords\n",
            "  - Removing Stopwords\n",
            "\n",
            "## Text Representation\n",
            "\n",
            "### Bag of Words\n",
            "\n",
            "- Bag of Words\n",
            "  - Count Vectorization\n",
            "  - TF-IDF Vectorization\n",
            "\n",
            "### Word Embeddings\n",
            "\n",
            "- Word Embeddings\n",
            "  - Word2Vec\n",
            "  - GloVe\n",
            "\n",
            "## Text Classification\n",
            "\n",
            "### Supervised Learning\n",
            "\n",
            "- Supervised Learning\n",
            "  - Naive Bayes\n",
            "  - Support Vector Machines\n",
            "\n",
            "### Unsupervised Learning\n",
            "\n",
            "- Unsupervised Learning\n",
            "  - K-Means Clustering\n",
            "  - Hierarchical Clustering\n",
            "----------\n",
            "\n",
            "sub page 20\n",
            "\n",
            "\n",
            "# Natural Language Processing\n",
            "\n",
            "## Transformer-based Models\n",
            "\n",
            "### Rethinking Attention with Performers\n",
            "- Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \n",
            "\n",
            "### Revisiting Transformer-based Models for Long Document Classification\n",
            "- Xiang Dai, Ilias Chalkidis, Sune Darkner, and Desmond Elliott\n",
            "\n",
            "### Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\n",
            "- Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov\n",
            "----------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}